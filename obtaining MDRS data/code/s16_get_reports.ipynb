{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this script gets all reports from 2016-2017 field season (s16)\n",
    "\n",
    "#problem: reports are grouped by categories (eva, commander, etc) and not by groups. I can not automatically get the crew #. but I can get dates and later assign crew names using them later.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "from bs4 import Comment\n",
    "import requests\n",
    "import urllib.request\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#different report landing pages; highest page # is 10.\n",
    "# #first page does not have page number; ../page/1 gives 404 error.  \n",
    "#start from 2; then add first pages to it (categories + good_links)\n",
    "\n",
    "categories = ['http://mdrs2016.marssociety.org/category/daily-summary', \n",
    "'http://mdrs2016.marssociety.org/category/eva',\n",
    "'http://mdrs2016.marssociety.org/category/commander',\n",
    "'http://mdrs2016.marssociety.org/category/journalist',\n",
    "'http://mdrs2016.marssociety.org/category/science',\n",
    "'http://mdrs2016.marssociety.org/category/greenhab-report',\n",
    "'http://mdrs2016.marssociety.org/category/creative-report'\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# url = urllib.request.urlopen('http://mdrs2016.marssociety.org/category/eva').read()\n",
    "# soup = bs(url, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop thru categories, try to open pages and get all working links to reports (../page/#)\n",
    "\n",
    "good_links = []\n",
    "brokenlinks = [] #for checking whether good links were not thrown out\n",
    "for c in categories:\n",
    "    for i in range (2,11): # last page 11 \n",
    "        try:\n",
    "            url = urllib.request.urlopen(c + \"/page/\" + str(i))\n",
    "            good_links.append(c + \"/page/\" + str(i))\n",
    "        except urllib.error.HTTPError as exception: #404 error exception \n",
    "            brokenlinks.append(c + \"/page/\" + str(i))\n",
    "        print (i)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "good_l = good_links  + categories\n",
    "good_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting date; other parts of soup are the same as 2017-2019 data.  \n",
    "\n",
    "\n",
    "# #soup.article.select(\".entry-content\n",
    "# #class_list = [\"fa fa-clock-o\"]; need to get text after for the date. \n",
    "\n",
    "\n",
    "# for comment in soup.find_all(text=lambda text: isinstance(text, Comment)):\n",
    "#     comment = soup.article.find_all(text=lambda text: isinstance(text, Comment))[1]\n",
    "     \n",
    "\n",
    "# # comment = soup.article.find_all(text=lambda text: isinstance(text, Comment))[1]\n",
    "\n",
    "\n",
    "# # comment = soup.find(text=lambda text:isinstance(text, Comment))\n",
    "# commentsoup = bs(comment , 'lxml')\n",
    "\n",
    "# #commentsoup.p.span.find_next\n",
    "# #commentsoup.p.find(class_=\"fa fa-clock-o\").next_sibling\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(soup):\n",
    "    date=[]\n",
    "    header = []\n",
    "    body = [] \n",
    "    html = []\n",
    "    for i in range(len(soup.find_all(\"article\"))):\n",
    "        comment = soup.find_all(\"article\")[i].find_all(text=lambda text: isinstance(text, Comment))[1]\n",
    "        commentsoup = bs(comment, 'lxml')\n",
    "        date.append (commentsoup.p.find(class_=\"fa fa-clock-o\").next_sibling)\n",
    "        header.append(soup.find_all(\"article\")[i].select(\".entry-title\")[0].get_text())\n",
    "        body.append(soup.find_all(\"article\")[i].select(\".entry-content\")[0].get_text())\n",
    "        html.append(soup.find_all(\"article\")[i].select(\".entry-content\")[0])\n",
    "    return date, header, body, html \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date=[]\n",
    "header =[]\n",
    "body=[]\n",
    "html =[]\n",
    "link = []\n",
    "\n",
    "\n",
    "for l in good_l:\n",
    "    print(l)\n",
    "    url = urllib.request.urlopen(l).read()\n",
    "    soup = bs(url, 'lxml')\n",
    "    for i in range(len(soup.find_all(\"article\"))):\n",
    "        comment = soup.find_all(\"article\")[i].find_all(text=lambda text: isinstance(text, Comment))[1]\n",
    "        commentsoup = bs(comment, 'lxml')\n",
    "        date.append (commentsoup.p.find(class_=\"fa fa-clock-o\").next_sibling)\n",
    "        header.append(soup.find_all(\"article\")[i].select(\".entry-title\")[0].get_text())\n",
    "        body.append(soup.find_all(\"article\")[i].select(\".entry-content\")[0].get_text())\n",
    "        html.append(soup.find_all(\"article\")[i].select(\".entry-content\")[0])\n",
    "        link.append(l)\n",
    "\n",
    "\n",
    "                    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if all correct:\n",
    "len(date), len(body), len(link), len(html), len(header)\n",
    "\n",
    "\n",
    "date[0:10], body[0:10], link[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create panda dataframe and dump to csv\n",
    "\n",
    "reports = pd.DataFrame({'date': date, 'header': header, 'body':body, 'html': html, 'link': link})\n",
    "\n",
    "#sort by date \n",
    "reports['date'] =pd.to_datetime(reports.date)\n",
    "\n",
    "sorted = reports.sort_values(by = ['date'])\n",
    "\n",
    "sorted.to_csv(r's16_reports.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
