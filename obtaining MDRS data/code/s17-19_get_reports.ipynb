{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this script gets all reports from 2017-2020 field seasons (s17, s18,s19)\n",
    "#together bc the same page structure now -- going back to 2017. \n",
    "\n",
    "#problem: reports are grouped by categories (eva, commander, etc) and not by groups. I can not automatically get the crew #. but I can get dates and later assign crew names using them. \n",
    "#output 's17-19_nocrew.csv' in the same directory as code. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import urllib.request\n",
    "import csv\n",
    "import urllib.request\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categories that i need to process:\n",
    "# http://mdrs.marssociety.org/category/astronomy-report/page/1/  -- page 15\n",
    "#http://mdrs.marssociety.org/category/commander-report/page/1/ -- page/13/\n",
    "#http://mdrs.marssociety.org/category/creative-report/ \n",
    "# http://mdrs.marssociety.org/category/eva-report/ -- page 32\n",
    "# http://mdrs.marssociety.org/category/greenhab-report/page/1/ -- page 42\n",
    "# http://mdrs.marssociety.org/category/journalist-report/ -- page 36\n",
    "# http://mdrs.marssociety.org/category/mission-plan/ --2\n",
    "# http://mdrs.marssociety.org/category/mission-summary/ -- 4\n",
    "#http://mdrs.marssociety.org/category/operations-report/  --52\n",
    "#http://mdrs.marssociety.org/category/research-report/ \n",
    "# http://mdrs.marssociety.org/category/science-report/  --14\n",
    "# http://mdrs.marssociety.org/category/sol-summary/ -- 48\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put them all into a single file\n",
    "#can loop thru list: try and except. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['http://mdrs.marssociety.org/category/astronomy-report/page/', \n",
    "'http://mdrs.marssociety.org/category/commander-report/page/',\n",
    "'http://mdrs.marssociety.org/category/creative-report/page/', \n",
    "'http://mdrs.marssociety.org/category/eva-report/page/',\n",
    "'http://mdrs.marssociety.org/category/greenhab-report/page/',\n",
    "'http://mdrs.marssociety.org/category/journalist-report/page/',\n",
    "'http://mdrs.marssociety.org/category/mission-plan/page/',\n",
    "'http://mdrs.marssociety.org/category/mission-summary/page/', \n",
    "'http://mdrs.marssociety.org/category/operations-report/page/',  \n",
    "'http://mdrs.marssociety.org/category/research-report/page/', \n",
    "'http://mdrs.marssociety.org/category/science-report/page/',\n",
    "'http://mdrs.marssociety.org/category/sol-summary/page'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop thru categories, try to open pages and get all working links to reports (../page/#)\n",
    "\n",
    "good_links = []\n",
    "brokenlinks = []\n",
    "for i in range (1,53): # last page 52 for operations reports, other categories have less pages\n",
    "    for c in categories:\n",
    "        try:\n",
    "            url = urllib.request.urlopen(c + str(i))\n",
    "            good_links.append(c + str(i))\n",
    "        except urllib.error.HTTPError as exception: #404 error exception \n",
    "            brokenlinks.append(c + str(i))\n",
    "    print (i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brokenlinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " url = urllib.request.urlopen('http://mdrs.marssociety.org/category/science-report/page/53')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# http://mdrs.marssociety.org/category/eva-report/page/32\n",
    "url = urllib.request.urlopen('http://mdrs.marssociety.org/category/eva-report/page/32').read()\n",
    "soup = bs(url, 'lxml')\n",
    "\n",
    "soup.article.select(\".entry-content\")[0]\n",
    "\n",
    "html =[]\n",
    "\n",
    "for i in range(len(soup.find_all(\"article\"))):\n",
    "    html.append(soup.find_all(\"article\")[i].select(\".entry-content\")[0])\n",
    "\n",
    "\n",
    "\n",
    "print(html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(url_link):\n",
    "    \n",
    "    \n",
    "    date=[]\n",
    "    header = []\n",
    "    body = [] \n",
    "    html = []\n",
    "    link = []\n",
    "\n",
    "    \n",
    "    url = urllib.request.urlopen(url_link).read()\n",
    "    soup = bs(url, 'lxml')\n",
    "    for i in range(len(soup.find_all(\"article\"))):\n",
    "        date.append(soup.find_all(\"article\")[i].time.get_text())\n",
    "        header.append(soup.find_all(\"article\")[i].select(\".entry-title\")[0].get_text())\n",
    "        body.append(soup.find_all(\"article\")[i].select(\".entry-content\")[0].get_text())\n",
    "        html.append(soup.find_all(\"article\")[i].select(\".entry-content\")[0])\n",
    "        link.append(url_link)\n",
    "\n",
    "    return date, header, body, html, link\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ['http://mdrs.marssociety.org/category/commander-report/page/1', 'http://mdrs.marssociety.org/category/commander-report/page/2', 'http://mdrs.marssociety.org/category/commander-report/page/3']\n",
    "print(test)\n",
    "\n",
    "for t in test:\n",
    "    url = urllib.request.urlopen(t).read()\n",
    "    print(url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date=[]\n",
    "header =[]\n",
    "body=[]\n",
    "html =[]\n",
    "link = []\n",
    "\n",
    "\n",
    "for t in test:\n",
    "    date.append(parse(t)[0])\n",
    "    header.append(parse(t)[1])\n",
    "    body.append(parse(t)[2])\n",
    "    html.append(parse(t)[3])\n",
    "    link.append(parse(t)[4])\n",
    "\n",
    "\n",
    "\n",
    "for d in range(len(date)):\n",
    "    print (date[d], header[d], body[d], html[d], link[d])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(date)):\n",
    "    print(len(date[i]), len(body[i]), len(link[i]))\n",
    "\n",
    "\n",
    "print (len(date[1]), len(body[1]), len(link[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date=[]\n",
    "header =[]\n",
    "body=[]\n",
    "html =[]\n",
    "link = []\n",
    "\n",
    "\n",
    "for l in good_links:\n",
    "    print(l)\n",
    "    url = urllib.request.urlopen(l).read()\n",
    "    soup = bs(url, 'lxml')\n",
    "    for i in range(len(soup.find_all(\"article\"))):\n",
    "        if soup.find_all(\"article\")[i].time:\n",
    "            date.append(soup.find_all(\"article\")[i].time.get_text())\n",
    "        else:\n",
    "            date.append(\"none\")\n",
    "        if soup.find_all(\"article\")[i].select(\".entry-title\"):     \n",
    "            header.append(soup.find_all(\"article\")[i].select(\".entry-title\")[0].get_text())\n",
    "        else:\n",
    "            header.append(\"none\") \n",
    "        if  soup.find_all(\"article\")[i].select(\".entry-content\"):\n",
    "\n",
    "            body.append(soup.find_all(\"article\")[i].select(\".entry-content\")[0].get_text())\n",
    "            html.append(soup.find_all(\"article\")[i].select(\".entry-content\")[0])\n",
    "        else:\n",
    "            body.append(\"none\")\n",
    "            html.append(\"none\")    \n",
    "        link.append(l)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "                    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(date), len(body), len(link), len(html), len(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create panda dataframe and dump to csv\n",
    "\n",
    "reports = pd.DataFrame({'date': date, 'header': header, 'body':body, 'html': html, 'link': link})\n",
    "\n",
    "#sort by date \n",
    "reports['date'] =pd.to_datetime(reports.date)\n",
    "\n",
    "sorted = reports.sort_values(by = ['date'])\n",
    "\n",
    "sorted.to_csv(r's17-19_nocrew.csv')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}