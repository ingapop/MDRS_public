{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code gets reports from fall 2014 - spring 2016 crews (s14-15). Crews 142-169. \n",
    "#https://sites.google.com/a/marssociety.org/mdrs2013/crew-reports landing site for crew 142-169 reports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import urllib.request\n",
    "import csv\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = urllib.request.urlopen('https://sites.google.com/a/marssociety.org/mdrs2013/crew-reports').read()\n",
    "soup = bs(url, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#links that have crew reports have this structure: \n",
    "#.../crew-xxx/0000-blablareport\n",
    "#get rid of bottom links that have google site stuff and the one that links to previous seasons \n",
    "\n",
    "report_links = []\n",
    "\n",
    "\n",
    "for i in range (len(soup.find_all (\"a\"))):\n",
    "    h = soup.find_all(\"a\")[i]['href']\n",
    "    if 'report' in str(h) and 'crew' in str(h) and 'accounts.google.com' not in str(h) and 'previous-field-seasons' not in str(h):\n",
    "        report_links.append(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = urllib.request.urlopen('https://sites.google.com/a/marssociety.org/mdrs2013/home/crew-169-team-peru-ivi/0513-eva14report').read()\n",
    "evasoup = bs(url, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in evasoup.find_all(\"a\"):\n",
    "    print(a.attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get html\n",
    "#soup.find(class_=\"sites-canvas-main\")\n",
    "\n",
    "#get body\n",
    "#soup.find(class_=\"sites-canvas-main\").get_text()\n",
    "\n",
    "#get date\n",
    "#date=title.split(\"-\")[0] \n",
    "\n",
    "#get title\n",
    "#title = soup.title.get_text() \n",
    "\n",
    "#get crew number \n",
    "\n",
    "test_l = report_links[103]\n",
    "print(test_l)\n",
    "test_name = test_l.split(\"/\")[-2]\n",
    "\n",
    "print(test_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in report_links:\n",
    "    print(l.split(\"/\")[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crew = []\n",
    "date = []\n",
    "header = []\n",
    "body = []\n",
    "html = []\n",
    "links = [] \n",
    "missing_links = []\n",
    "for l in report_links:\n",
    "    try:\n",
    "        print(l)\n",
    "        url = urllib.request.urlopen(l).read()\n",
    "        rsoup = bs(url, 'lxml')\n",
    "        #get crew name \n",
    "        crew.append(l.split(\"/\")[-2])  \n",
    "        # header\n",
    "        title = rsoup.title.get_text() \n",
    "        header.append(title)\n",
    "        #date \n",
    "        date.append(title.split('-')[0])\n",
    "        #text body\n",
    "        htmltext = rsoup.find(class_=\"sites-canvas-main\")\n",
    "        body.append (htmltext.get_text())\n",
    "        #html\n",
    "        html.append(htmltext)\n",
    "        #link\n",
    "        links.append(l)\n",
    "    except urllib.error.HTTPError as exception:\n",
    "        missing_links.append(l)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(links), len(body), len(html), len(header), len(date), len(crew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write into pandas df. \n",
    "df = pd.DataFrame({'crew': crew, 'date': date, 'header': header, 'body':body, 'html': html, 'link': links})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix crew-xx into xx\n",
    "for i in range(len(df)):\n",
    "    df['crew'].iloc[i] = df['crew'].iloc[i].split(\"crew-\")[1]\n",
    "   # print(i, df['crew'].iloc[i])\n",
    "\n",
    "#fix crew -- continue\n",
    "for i in range(len(df)):\n",
    "    df['crew'].iloc[i] = df['crew'].iloc[i].split(\"-\")[0]\n",
    "   # print(i, df['crew'].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('s14-15_reports.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
